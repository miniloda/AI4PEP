{
 "cells": [
  {
   "cell_type": "code",
   "id": "84cdb398-deca-417c-a06d-ec57f605f181",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T10:52:46.941014Z",
     "start_time": "2024-07-22T10:52:45.746307Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "import json\n",
    "import jsonlines\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "c5518f0a-0e11-47dc-b715-f9051abe47a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T10:52:46.959920Z",
     "start_time": "2024-07-22T10:52:46.945996Z"
    }
   },
   "source": [
    "def convert_jsonl_to_docbin(input_file, output_train_file, output_test_file, model=\"en_core_web_trf\", relation_labels=[]):\n",
    "    \"\"\"\n",
    "    Convert JSONL file to Spacy DocBin file for training and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input JSONL file.\n",
    "        output_train_file (str): Path to save the training DocBin file.\n",
    "        output_test_file (str): Path to save the testing DocBin file.\n",
    "        model (str, optional): Spacy model to use. Defaults to \"en_core_web_trf\".\n",
    "        relation_labels (list, optional): List of relation labels. Defaults to an empty list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load Spacy model\n",
    "    nlp = spacy.load(model)\n",
    "\n",
    "    # Read JSONL file\n",
    "    annotations = pd.read_json(input_file, lines=True)\n",
    "\n",
    "    # Set extension for Doc object\n",
    "    Doc.set_extension(\"rel\", default={}, force=True)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(annotations[\"text\"], annotations[\"entities\"], test_size=0.33, random_state=42)\n",
    "\n",
    "    def create_docbin(texts, entities_list, output_file):\n",
    "        \"\"\"\n",
    "        Convert text and entities list to Spacy DocBin format and save to file.\n",
    "        \n",
    "        Args:\n",
    "            texts (pd.Series): Series of texts.\n",
    "            entities_list (pd.Series): Series of entities.\n",
    "            output_file (str): Path to save the DocBin file.\n",
    "        \"\"\"\n",
    "        db = DocBin()\n",
    "        entry_id_counter = 1  # Counter for entry IDs\n",
    "        entity_id_counter = [1]  # Mutable list to maintain the global counter for entities\n",
    "\n",
    "        # List to store IDs with issues\n",
    "        ids_with_issues = []\n",
    "\n",
    "        # Iterate over each annotation\n",
    "        for text, entities in zip(texts, entities_list):\n",
    "            try:\n",
    "                span_starts = set()\n",
    "                entities_out = []\n",
    "                span_end_to_start = {}\n",
    "\n",
    "                # Create a Doc object from the text using the Spacy model\n",
    "                doc = nlp.make_doc(text)\n",
    "\n",
    "                # Keep track of referenced entities to avoid duplicates\n",
    "                ref_entities = []\n",
    "\n",
    "                # Process each entity\n",
    "                for entity in entities:\n",
    "                    start = entity[\"start_offset\"]\n",
    "                    end = entity[\"end_offset\"]\n",
    "\n",
    "                    if (start, end) not in ref_entities:\n",
    "                        ref_entities.append((start, end))\n",
    "                        label = entity[\"label\"]\n",
    "\n",
    "                        # Create a span from the character offsets and label\n",
    "                        span = doc.char_span(start, end, label=label)\n",
    "\n",
    "                        if span is None:\n",
    "                            print(f\"Skipping entity for {label} ({start},{end})\")\n",
    "                        else:\n",
    "                            entities_out.append(span)\n",
    "                            span_end_to_start[entity[\"id\"]] = entity[\"id\"]\n",
    "                            span_starts.add(entity[\"id\"])\n",
    "\n",
    "                # Create an empty dictionary to store relations\n",
    "                rels = {}\n",
    "                for x1 in span_starts:\n",
    "                    for x2 in span_starts:\n",
    "                        rels[(x1, x2)] = {}\n",
    "\n",
    "                # Process relations\n",
    "                relations = []  # Relations can be processed similarly if available\n",
    "                for relation in relations:\n",
    "                    start = span_end_to_start.get(relation[\"from_id\"], None)\n",
    "                    end = span_end_to_start.get(relation[\"to_id\"], None)\n",
    "                    label = relation[\"type\"]\n",
    "\n",
    "                    if start is not None and end is not None:\n",
    "                        if label not in rels[(start, end)]:\n",
    "                            rels[(start, end)][label] = 1.0\n",
    "\n",
    "                # Fill in zeros where relation data is missing\n",
    "                for x1 in span_starts:\n",
    "                    for x2 in span_starts:\n",
    "                        for label in relation_labels:\n",
    "                            if label not in rels[(x1, x2)]:\n",
    "                                rels[(x1, x2)][label] = 0.0\n",
    "\n",
    "                # Assign the entities and relations to the Doc object\n",
    "                doc.ents = entities_out\n",
    "                doc._.rel = rels\n",
    "\n",
    "                # Add the processed document to the DocBin object\n",
    "                db.add(doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing ID {entry_id_counter}: {str(e)}\")\n",
    "                ids_with_issues.append(entry_id_counter)\n",
    "            \n",
    "            entry_id_counter += 1  # Increment entry ID after each entry\n",
    "\n",
    "        # Save the DocBin object to disk\n",
    "        db.to_disk(output_file)\n",
    "\n",
    "        return db, ids_with_issues\n",
    "\n",
    "    # Create DocBin files for training and testing data\n",
    "    create_docbin(X_train, y_train, output_train_file)\n",
    "    create_docbin(X_test, y_test, output_test_file)\n",
    "\n",
    "    print(f\"Training data saved to {output_train_file}\")\n",
    "    print(f\"Testing data saved to {output_test_file}\")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "5af02818-22ec-4652-a805-891e5dddb0de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T10:52:47.183347Z",
     "start_time": "2024-07-22T10:52:46.962350Z"
    }
   },
   "source": [
    "ner_annotations = pd.read_json(\"data/output.jsonl\", lines=True)\n",
    "ner_annotations.drop(\"tag\", axis=1, inplace=True)\n",
    "ner_annotations.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id                                               text  \\\n",
       "0   1  BP spikes 1 day PTC, patient has dizziness. BO...   \n",
       "1   2  follow up occ cough no nasal catarrh, no dob o...   \n",
       "2   3  general check up Patient came in for general c...   \n",
       "3   4              eye redness  Bacterial conjunctivitis   \n",
       "4   5  hypogastric pain 3 days PTC patient had hypoga...   \n",
       "\n",
       "                                            entities relations Comments  \n",
       "0                                                 []        []       []  \n",
       "1  [{'id': 1, 'label': 'ILI', 'start_offset': 48,...        []       []  \n",
       "2                                                 []        []       []  \n",
       "3                                                 []        []       []  \n",
       "4                                                 []        []       []  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>relations</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BP spikes 1 day PTC, patient has dizziness. BO...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>follow up occ cough no nasal catarrh, no dob o...</td>\n",
       "      <td>[{'id': 1, 'label': 'ILI', 'start_offset': 48,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>general check up Patient came in for general c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>eye redness  Bacterial conjunctivitis</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>hypogastric pain 3 days PTC patient had hypoga...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "f4975aab-186e-454f-af23-8c8082f0760d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T10:52:55.576444Z",
     "start_time": "2024-07-22T10:52:47.184991Z"
    }
   },
   "source": [
    "convert_jsonl_to_docbin(\n",
    "    input_file='data/output.jsonl',\n",
    "    output_train_file='data/train_data.spacy',\n",
    "    output_test_file='data/test_data.spacy',\n",
    "    model = 'en_core_web_lg',\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity for ILI (72,100)\n",
      "Skipping entity for ILI (91,100)\n",
      "Skipping entity for ILI (404,408)\n",
      "Skipping entity for ILI (13,18)\n",
      "Skipping entity for ILI (203,208)\n",
      "Error processing ID 272: [E1010] Unable to set entity information for token 39 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (214,218)\n",
      "Skipping entity for ILI (13,18)\n",
      "Error processing ID 505: [E1010] Unable to set entity information for token 29 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (71,76)\n",
      "Skipping entity for ILI (13,18)\n",
      "Error processing ID 600: [E1010] Unable to set entity information for token 18 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Error processing ID 631: [E1010] Unable to set entity information for token 49 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (11,16)\n",
      "Error processing ID 898: [E1010] Unable to set entity information for token 20 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (6,11)\n",
      "Skipping entity for ILI (0,5)\n",
      "Error processing ID 1327: [E1010] Unable to set entity information for token 19 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (13,18)\n",
      "Error processing ID 1399: [E1010] Unable to set entity information for token 1 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (13,18)\n",
      "Skipping entity for ILI (13,18)\n",
      "Skipping entity for ILI (62,67)\n",
      "Skipping entity for ILI (6,11)\n",
      "Skipping entity for ILI (184,212)\n",
      "Skipping entity for ILI (203,212)\n",
      "Skipping entity for ILI (0,5)\n",
      "Error processing ID 1855: [E1010] Unable to set entity information for token 1 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (116,120)\n",
      "Error processing ID 1944: [E1010] Unable to set entity information for token 51 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (244,248)\n",
      "Error processing ID 73: [E1010] Unable to set entity information for token 2 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (6,11)\n",
      "Error processing ID 454: [E1010] Unable to set entity information for token 30 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (316,320)\n",
      "Error processing ID 609: [E1010] Unable to set entity information for token 1 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (14,19)\n",
      "Skipping entity for ILI (19,24)\n",
      "Skipping entity for ILI (41,46)\n",
      "Error processing ID 765: [E1010] Unable to set entity information for token 115 which is included in more than one span in entities, blocked, missing or outside.\n",
      "Skipping entity for ILI (16,21)\n",
      "Skipping entity for ILI (30,35)\n",
      "Skipping entity for ILI (23,28)\n",
      "Training data saved to data/train_data.spacy\n",
      "Testing data saved to data/test_data.spacy\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "440ef15e-bab7-48f4-84a1-1f45407e0574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T10:53:33.476909Z",
     "start_time": "2024-07-22T10:52:55.578826Z"
    }
   },
   "source": [
    "# Define Configurations\n",
    "os.system(\"python3 -m spacy init fill-config base_config.cfg experiment_config.cfg\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2m✔ Auto-filled config with all values\u001B[0m\n",
      "\u001B[38;5;2m✔ Saved config\u001B[0m\n",
      "experiment_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train experiment_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "80cae350-3761-4ad9-b62f-d3e418f2af27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:03:35.681366Z",
     "start_time": "2024-07-22T10:53:33.483576Z"
    }
   },
   "source": "os.system(\"python3 -m spacy train experiment_config.cfg --output output/v0 --paths.train data/train_data.spacy --paths.dev data/train_data.spacy\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4mℹ Saving to output directory: output/v0\u001B[0m\n",
      "\u001B[38;5;4mℹ Using CPU\u001B[0m\n",
      "\u001B[1m\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\n",
      "\u001B[38;5;2m✔ Initialized pipeline\u001B[0m\n",
      "\u001B[1m\n",
      "============================= Training pipeline =============================\u001B[0m\n",
      "\u001B[38;5;4mℹ Pipeline: ['ner']\u001B[0m\n",
      "\u001B[38;5;4mℹ Initial learn rate: 0.0001\u001B[0m\n",
      "E    #       LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  --------  ------  ------  ------  ------\n",
      "  0       0     26.86    0.00    0.00    0.00    0.00\n",
      "  0     200   2422.84   19.54   74.02   11.26    0.20\n",
      "  1     400    708.39   95.99   97.39   94.63    0.96\n",
      "  1     600    240.69   98.61   99.58   97.65    0.99\n",
      "  2     800    157.45   98.86   99.51   98.21    0.99\n",
      "  3    1000    130.11   98.94   99.10   98.77    0.99\n",
      "  5    1200    123.14   99.18   99.55   98.81    0.99\n",
      "  6    1400    124.34   99.10   99.25   98.96    0.99\n",
      "  8    1600    139.87   99.66   99.66   99.66    1.00\n",
      " 11    1800    203.10   99.78   99.78   99.78    1.00\n",
      " 14    2000    204.86   99.83   99.78   99.89    1.00\n",
      " 18    2200     75.86   99.87   99.81   99.93    1.00\n",
      " 23    2400     46.22   99.96   99.93  100.00    1.00\n",
      " 27    2600     24.12   99.98   99.96  100.00    1.00\n",
      " 32    2800     13.73  100.00  100.00  100.00    1.00\n",
      " 37    3000     13.06  100.00  100.00  100.00    1.00\n",
      " 42    3200      4.22  100.00  100.00  100.00    1.00\n",
      " 46    3400      7.91  100.00  100.00  100.00    1.00\n",
      " 51    3600      3.26  100.00  100.00  100.00    1.00\n",
      " 56    3800      2.31  100.00  100.00  100.00    1.00\n",
      " 61    4000      2.13  100.00  100.00  100.00    1.00\n",
      " 65    4200      6.01  100.00  100.00  100.00    1.00\n",
      " 70    4400      7.10  100.00  100.00  100.00    1.00\n",
      "\u001B[38;5;2m✔ Saved pipeline to output directory\u001B[0m\n",
      "output/v0/model-last\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "81c9ee75-8fa9-4290-8dd0-46f935250c94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:07:53.911597Z",
     "start_time": "2024-07-22T11:07:48.319380Z"
    }
   },
   "source": [
    "import spacy\n",
    "model = spacy.load(\"output/v0/model-best/\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "9faa2d94-1139-4f5e-9625-03e7d0700c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:07:55.404631Z",
     "start_time": "2024-07-22T11:07:54.918740Z"
    }
   },
   "source": "text = model(ner_annotations[\"text\"][1])",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ner_annotations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m text \u001B[38;5;241m=\u001B[39m model(\u001B[43mner_annotations\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m1\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ner_annotations' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "6d7ab965-7390-4227-91aa-22cd9aff3ea2",
   "metadata": {},
   "source": [
    "doc = model(text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24388645-fb8e-424b-8cad-dc476c8e289e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:07:57.294608Z",
     "start_time": "2024-07-22T11:07:57.256535Z"
    }
   },
   "source": [
    "entities = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entity = {\n",
    "        \"start_index\": ent.start_char,\n",
    "        \"end_index\": ent.end_char,\n",
    "        \"text\": ent.text,\n",
    "        \"entity_type\": ent.label_\n",
    "    }\n",
    "    entities.append(entity)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m entities \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ent \u001B[38;5;129;01min\u001B[39;00m \u001B[43mdoc\u001B[49m\u001B[38;5;241m.\u001B[39ments:\n\u001B[1;32m      4\u001B[0m     entity \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      5\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstart_index\u001B[39m\u001B[38;5;124m\"\u001B[39m: ent\u001B[38;5;241m.\u001B[39mstart_char,\n\u001B[1;32m      6\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend_index\u001B[39m\u001B[38;5;124m\"\u001B[39m: ent\u001B[38;5;241m.\u001B[39mend_char,\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: ent\u001B[38;5;241m.\u001B[39mtext,\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mentity_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: ent\u001B[38;5;241m.\u001B[39mlabel_\n\u001B[1;32m      9\u001B[0m     }\n\u001B[1;32m     10\u001B[0m     entities\u001B[38;5;241m.\u001B[39mappend(entity)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'doc' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "978e3bc0-ef31-4983-9177-47ae1dcca484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:07:57.972937Z",
     "start_time": "2024-07-22T11:07:57.969629Z"
    }
   },
   "source": [
    "for entity in entities:\n",
    "    print(entity)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:07:59.373282Z",
     "start_time": "2024-07-22T11:07:59.366723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(true_entities, pred_entities):\n",
    "    \"\"\"\n",
    "    Compute True Positives (TP), False Positives (FP), and False Negatives (FN),\n",
    "    as well as Precision, Recall, and F1 Score.\n",
    "\n",
    "    Args:\n",
    "        true_entities (list): List of true entities as tuples (text, label).\n",
    "        pred_entities (list): List of predicted entities as tuples (text, label).\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with TP, FP, FN, Precision, Recall, and F1 Score.\n",
    "    \"\"\"\n",
    "    true_entities_set = set(true_entities)\n",
    "    pred_entities_set = set(pred_entities)\n",
    "\n",
    "    # True Positives (TP): Predicted entities that are also in true entities\n",
    "    tp = len(true_entities_set.intersection(pred_entities_set))\n",
    "\n",
    "    # False Positives (FP): Predicted entities that are not in true entities\n",
    "    fp = len(pred_entities_set - true_entities_set)\n",
    "\n",
    "    # False Negatives (FN): True entities that are not in predicted entities\n",
    "    fn = len(true_entities_set - pred_entities_set)\n",
    "\n",
    "    # Calculate Precision, Recall, and F1 Score\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return {\n",
    "        'True Positive': tp,\n",
    "        'False Positive': fp,\n",
    "        'False Negative': fn,\n",
    "        'True Negative': 'Not Computed',  # Typically not used in NER evaluation\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1_score,\n",
    "        \n",
    "    }\n",
    "\n"
   ],
   "id": "cbd802efe7cd68f1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:08:00.157886Z",
     "start_time": "2024-07-22T11:08:00.153766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(test_docs, predictions):\n",
    "    true_entities = []\n",
    "    pred_entities = []\n",
    "    \n",
    "    # Collect all true and predicted entities for comparison\n",
    "    for doc, (_, preds) in zip(test_docs, predictions):\n",
    "        true_entities.extend([(ent.text, ent.label_) for ent in doc.ents])\n",
    "        pred_entities.extend(preds)\n",
    "\n",
    "    metrics = compute_metrics(true_entities, pred_entities)\n",
    "    \n",
    "    print(\"Metrics:\")\n",
    "    print(f\"True Positives (TP): {metrics['True Positive']}\")\n",
    "    print(f\"False Positives (FP): {metrics['False Positive']}\")\n",
    "    print(f\"False Negatives (FN): {metrics['False Negative']}\")\n",
    "    print(f\"True Negatives (TN): {metrics['True Negative']}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.2f}\")\n",
    "    print(f\"Recall: {metrics['Recall']:.2f}\")\n",
    "    print(f\"F1 Score: {metrics['F1 Score']:.2f}\")\n"
   ],
   "id": "5f4b403540fa0a46",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T11:09:53.118835Z",
     "start_time": "2024-07-22T11:09:52.952782Z"
    }
   },
   "cell_type": "code",
   "source": "test_docs = spacy.load(\"/home/miniloda/Documents/GitHub/ai4pep/NER/Model/data/test_data.spacy\")\n",
   "id": "43c68b55d6dea1e3",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read meta.json from /home/miniloda/Documents/GitHub/ai4pep/NER/Model/data/test_data.spacy",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m test_docs \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/miniloda/Documents/GitHub/ai4pep/NER/Model/data/test_data.spacy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/spacy/__init__.py:51\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     28\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     35\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py:467\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    465\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_package(name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    466\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m Path(name)\u001B[38;5;241m.\u001B[39mexists():  \u001B[38;5;66;03m# path to model data directory\u001B[39;00m\n\u001B[0;32m--> 467\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_model_from_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    468\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexists\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# Path or Path-like to model data\u001B[39;00m\n\u001B[1;32m    469\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_path(name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py:535\u001B[0m, in \u001B[0;36mload_model_from_path\u001B[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    533\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE052\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mmodel_path))\n\u001B[1;32m    534\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m meta:\n\u001B[0;32m--> 535\u001B[0m     meta \u001B[38;5;241m=\u001B[39m \u001B[43mget_model_meta\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    536\u001B[0m config_path \u001B[38;5;241m=\u001B[39m model_path \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig.cfg\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    537\u001B[0m overrides \u001B[38;5;241m=\u001B[39m dict_to_dot(config, for_overrides\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py:929\u001B[0m, in \u001B[0;36mget_model_meta\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    923\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Get model meta.json from a directory path and validate its contents.\u001B[39;00m\n\u001B[1;32m    924\u001B[0m \n\u001B[1;32m    925\u001B[0m \u001B[38;5;124;03mpath (str / Path): Path to model directory.\u001B[39;00m\n\u001B[1;32m    926\u001B[0m \u001B[38;5;124;03mRETURNS (Dict[str, Any]): The model's meta data.\u001B[39;00m\n\u001B[1;32m    927\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    928\u001B[0m model_path \u001B[38;5;241m=\u001B[39m ensure_path(path)\n\u001B[0;32m--> 929\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mload_meta\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmeta.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py:889\u001B[0m, in \u001B[0;36mload_meta\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    887\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE052\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mpath\u001B[38;5;241m.\u001B[39mparent))\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m path\u001B[38;5;241m.\u001B[39mexists() \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m path\u001B[38;5;241m.\u001B[39mis_file():\n\u001B[0;32m--> 889\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE053\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mpath\u001B[38;5;241m.\u001B[39mparent, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta.json\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    890\u001B[0m meta \u001B[38;5;241m=\u001B[39m srsly\u001B[38;5;241m.\u001B[39mread_json(path)\n\u001B[1;32m    891\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m setting \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlang\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversion\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "\u001B[0;31mOSError\u001B[0m: [E053] Could not read meta.json from /home/miniloda/Documents/GitHub/ai4pep/NER/Model/data/test_data.spacy"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b714d290f1793dc9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
